# LLaDA Model Configuration - Small (for testing/debugging)

[model]
vocab_size = 1000
max_sequence_length = 64
embedding_dimension = 64
number_of_heads = 2
number_of_layers = 2
mask_token_id = -1  # -1 means append [MASK] token to vocab

[model.dimensions]
time_dimension = 32
state_dimension = 64  # -1 means use embedding_dimension

[model.attention]
window_size = 5

[model.oscillator]
min_frequency = 0.1
max_frequency = 10.0
default_time_step = 0.1

[generation]
default_num_steps = 10
default_temperature = 1.0

[training]
mask_schedule = "uniform"  # "uniform" or "cosine"
