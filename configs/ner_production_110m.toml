# OssammaNER Production Configuration - 75M Parameters
# Optimized for RTX 5090 (32GB VRAM)

# =============================================================================
# Model Architecture (~75M Parameters - Production)
# =============================================================================
[model]
vocab_size = 32000              # Standard BPE vocabulary
max_sequence_length = 256       # Context length
embedding_dimension = 640       # Reduced from 768
number_of_heads = 10            # 64 dims per head
number_of_layers = 10           # Reduced from 12
num_labels = 17                 # 8 entity types × 2 (B/I) + O

[model.dimensions]
time_dimension = 192            # Reduced time conditioning
state_dimension = 640           # Match embedding dim

[model.attention]
window_size = 128               # Legacy default (see window_schedule for per-layer plan)

[[model.attention.window_schedule]]
percent = 0.70
window_size = 128

[[model.attention.window_schedule]]
percent = 0.30
window_size = 256

[model.oscillator]
min_frequency = 0.01            # Slow oscillations for long-range
max_frequency = 5.0             # Fast for word-level patterns
default_time_step = 0.05

[[model.oscillator.frequency_schedule]]
percent = 0.70
min_frequency = 1.0
max_frequency = 20.0

[[model.oscillator.frequency_schedule]]
percent = 0.30
min_frequency = 0.2
max_frequency = 6.0

[model.regularization]
dropout_rate = 0.1
label_smoothing = 0.1

[model.ablation]
use_output_gate = false           # Output gate removed (redundant with α-mixing)
use_ffn = true                    # SwiGLU FFN after α-mixing (Option E)
ffn_expansion = 1.333333          # 4/3 expansion factor (power-of-2 split dim)

# Alpha mixing ablation options (all disabled by default)
use_vector_gains = false          # Learnable per-dim gains s_g, s_l (+2d params/layer = 12.8K total)
use_per_head_alpha = false        # Per-head α instead of scalar (+d*h params/layer = 57.7K total)
use_branch_projections = false    # Full d→d projections per branch (+2d² params/layer = 8.2M total)

[parallelization]
use_parallel_scan = true          # GPU parallel associative scan (10-40× speedup)
chunk_size = 64                   # Chunk size for parallel processing

# =============================================================================
# Training Hyperparameters
# =============================================================================
[training]
batch_size = 8                  # Larger batch for 75M model on 32GB VRAM
gradient_accumulation_steps = 8  # Effective batch size = 64
learning_rate = 2e-4            # Standard LR for fresh training
min_learning_rate = 1e-6
warmup_steps = 1000             # Faster warmup
total_steps = 50000             # Training run
gradient_clip = 1.0
weight_decay = 0.01

[training.checkpoints]
eval_every = 250                # Evaluate frequently
log_every = 25                  # Progress updates
save_every = 1000               # Save checkpoint
push_every = 2000               # Push to Git more often

# =============================================================================
# Data Configuration
# =============================================================================
[data]
train_path = "data/ner/train.jsonl"
val_path = "data/ner/validation.jsonl"
test_path = "data/ner/test.jsonl"
max_len = 256
data_dir = "data/ner"

# =============================================================================
# Hardware Settings
# =============================================================================
[hardware]
device = "gpu"
mixed_precision = true          # Use Float16 for speed
num_workers = 4

# =============================================================================
# Git Push Settings
# =============================================================================
[git]
remote = "origin"
branch = "master"
checkpoint_dir = "checkpoints/ner_110m"
