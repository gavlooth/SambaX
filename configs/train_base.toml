# LLaDA Training Configuration - Base Model

# =============================================================================
# Model Architecture
# =============================================================================
[model]
vocab_size = 32000
max_sequence_length = 512
embedding_dimension = 512
number_of_heads = 8
number_of_layers = 12
mask_token_id = -1  # -1 = append [MASK] to vocab

[model.dimensions]
time_dimension = 128
state_dimension = 512

[model.attention]
window_size = 64

[model.oscillator]
min_frequency = 0.1
max_frequency = 10.0
default_time_step = 0.1

# =============================================================================
# Training Hyperparameters
# =============================================================================
[training]
batch_size = 32
learning_rate = 1e-4
min_learning_rate = 1e-6
warmup_steps = 2000
total_steps = 100000
gradient_clip = 1.0
mask_schedule = "cosine"  # "uniform" or "cosine"

[training.checkpoints]
eval_every = 1000
log_every = 100
save_every = 5000

# =============================================================================
# Generation Defaults
# =============================================================================
[generation]
default_num_steps = 20
default_temperature = 1.0

# =============================================================================
# Data (example paths - customize for your setup)
# =============================================================================
[data]
train_path = "data/train.txt"
val_path = "data/val.txt"
tokenizer = "bpe"  # or "char", "word"
