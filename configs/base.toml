# LLaDA Model Configuration - Base

[model]
vocab_size = 32000
max_sequence_length = 512
embedding_dimension = 512
number_of_heads = 8
number_of_layers = 12
mask_token_id = -1

[model.dimensions]
time_dimension = 128
state_dimension = 512

[model.attention]
window_size = 64

[model.oscillator]
min_frequency = 0.1
max_frequency = 10.0
default_time_step = 0.1

[generation]
default_num_steps = 20
default_temperature = 1.0

[training]
mask_schedule = "cosine"
